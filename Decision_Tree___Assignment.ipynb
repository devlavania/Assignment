{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Ans:- A Decision Tree is a flowchart-like model that splits data into branches based on feature values, ultimately leading to class labels for classification.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans:- Gini Impurity and Entropy measure how mixed the classes are in a node; decision trees use them to choose splits that create purer child nodes.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans:- Pre-Pruning: Stops tree growth early (e.g., limiting depth or minimum samples per split). Advantage: Prevents overfitting and reduces computation time.\n",
        "\n",
        "Post-Pruning: Grows the full tree first, then trims back branches. Advantage: Produces a simpler, more generalizable model after evaluating performance.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Ans:- Information Gain measures the reduction in impurity (entropy) after a split; it helps decision trees choose the feature and threshold that create the most informative, pure child nodes.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Ans:- Decision Trees are used in areas like medical diagnosis, credit risk assessment, customer segmentation, and fraud detection.\n",
        "\n",
        "Advantages: Easy to understand, handle both numerical and categorical data, require little data preprocessing.\n",
        "\n",
        "Limitations: Prone to overfitting, can be unstable with small data changes, less effective for very complex patterns compared to ensemble methods.\n",
        "\n",
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance and describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "Ans:-\n",
        "- **Handle missing values:**  \n",
        "  - **Assess:** Quantify missingness per feature.  \n",
        "  - **Impute:** Use median for numeric, most frequent for categorical; consider domain-informed imputation.  \n",
        "  - **Flag:** Add **missingness indicators** if patterns may be informative.\n",
        "\n",
        "- **Encode categorical features:**  \n",
        "  - **Low-cardinality:** **One-hot encoding**.  \n",
        "  - **High-cardinality:** **Target encoding** (with CV to reduce leakage) or **ordinal encoding** if natural order exists.\n",
        "\n",
        "- **Train a Decision Tree model:**  \n",
        "  - **Split data:** Train/validation/test with stratification.  \n",
        "  - **Fit:** DecisionTreeClassifier on preprocessed features; set a **class_weight='balanced'** if classes are imbalanced.\n",
        "\n",
        "- **Tune hyperparameters:**  \n",
        "  - **Parameters:** **max_depth**, **min_samples_split**, **min_samples_leaf**, **max_features**, **criterion** (gini/entropy).  \n",
        "  - **Search:** Use **GridSearchCV** or **RandomizedSearchCV** with stratified CV; optimize for **ROC-AUC** or **F1**.\n",
        "\n",
        "- **Evaluate performance and business value:**  \n",
        "  - **Metrics:** ROC-AUC, precision/recall, F1, confusion matrix; calibrate probabilities if needed.  \n",
        "  - **Business value:** Earlier risk identification, prioritized screenings, reduced costs via targeted interventions, transparent decisions to support clinicians, and measurable lift in detection rates with acceptable false-positive trade-offs."
      ],
      "metadata": {
        "id": "sxlrjzhkVSw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "dtc = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# Print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, dtc.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z-enG_qXdu5",
        "outputId": "9e34c673-9fa3-4511-b085-da1302523904"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dtc_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dtc_limited.fit(X_train, y_train)\n",
        "y_pred_limited = dtc_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.4f}\")\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no max_depth limit)\n",
        "dtc_full = DecisionTreeClassifier(random_state=42)\n",
        "dtc_full.fit(X_train, y_train)\n",
        "y_pred_full = dtc_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbab53QzX3-D",
        "outputId": "57bfddef-d310-4e3a-ad91-977aa8739b11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy of fully-grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing Dataset as an alternative to Boston Housing\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "dtr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtr.predict(X_test)\n",
        "\n",
        "# Print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, dtr.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLB_ZPxWYvdT",
        "outputId": "e929ba87-2227-4164-dd72-3b59cbb1a809"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280\n",
            "\n",
            "Feature Importances:\n",
            "  MedInc: 0.5235\n",
            "  HouseAge: 0.0521\n",
            "  AveRooms: 0.0494\n",
            "  AveBedrms: 0.0250\n",
            "  Population: 0.0322\n",
            "  AveOccup: 0.1390\n",
            "  Latitude: 0.0900\n",
            "  Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None], # None means no limit on depth\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize a Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model) from GridSearchCV\n",
        "best_dtc = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_best = best_dtc.predict(X_test)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Accuracy of the best model: {accuracy_best:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRmTAUhvY5Jt",
        "outputId": "6c9b8738-eac1-41a0-edf2-acf279aa84f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy of the best model: 1.0000\n"
          ]
        }
      ]
    }
  ]
}